{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./resources/images/header_logo.png)\n",
    "\n",
    "# Chapter 2 Getting started with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore **Spacy**, one of the most popular open-source NLP libraries. \n",
    "It is designed to hide away all of the complex algorithms and just give the user ready functions for text processing and analysis. It is also designed to work efficiently with other data science related libraries like *numpy* or *pandas*.\n",
    "\n",
    "Spacy chooses the most efficient algorithms for you. There is one drawback: you may not choose other algorithms. If you want to dive deeper and experiment with spacy, [**spacy documentation**](https://spacy.io/) is a great place to start.\n",
    "This section will focus on key text processing methods, namely:\n",
    "* Tokenization\n",
    "* Lemmatization\n",
    "* Stemming\n",
    "* Handling Stop Words\n",
    "* Vocabulary Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start using the spacy library, we also need to download the language model we will use. Language models are just large files that have information on specific languages, for example: words, stop_words, their vector representations (more in Chapter 3), lemmas, parts of speech (POS) and many more. Firstly, let's import spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to load an appropriate language model. For this course, we will be using a medium-size English language model with word embeddings called \"en_core_web_md\". It may take a while. Loading a model returns a spacy pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `nlp` is a default spacy pipeline. But, what is a pipeline?\n",
    "A pipeline in software engineering is a special toolbox, which stores different functions (tools) to execute on data in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 1 Spacy Pipeline\n",
    "Click [here](google.com) to watch on YT or run the cell below for in-app view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "publish() got an unexpected keyword argument 'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YouTubeVideo\n\u001b[0;32m      2\u001b[0m video \u001b[38;5;241m=\u001b[39m YouTubeVideo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m100\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200px\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\display_functions.py:305\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;66;03m# kwarg-specified metadata gets precedence\u001b[39;00m\n\u001b[0;32m    304\u001b[0m             _merge(md_dict, metadata)\n\u001b[1;32m--> 305\u001b[0m         publish_display_data(data\u001b[38;5;241m=\u001b[39mformat_dict, metadata\u001b[38;5;241m=\u001b[39mmd_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_id:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisplayHandle(display_id)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\display_functions.py:93\u001b[0m, in \u001b[0;36mpublish_display_data\u001b[1;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transient:\n\u001b[0;32m     91\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransient\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m transient\n\u001b[1;32m---> 93\u001b[0m display_pub\u001b[38;5;241m.\u001b[39mpublish(\n\u001b[0;32m     94\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     95\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     97\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: publish() got an unexpected keyword argument 'width'"
     ]
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(\"\")\n",
    "display(video, width=\"100%\", height=\"200px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./resources/images/spacy_pipeline.svg)\n",
    "\n",
    "Now that we have loaded the model and created a standard pipeline. Let's take a look what is inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x20d86139100>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x20d86139ac0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x20d859fef90>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x20d874fcb00>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x20d87504c80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x20de0bf9c10>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of different tools included in the pipeline. We will explore some of them in more depth now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Firstly, the plain text is broken into smallest units like individual words or terms (**tokenization**). We call these smallest chunks of text **tokens**. Tokens can take form of phrases, individual words, numerals, punctuation marks, whitespaces and more. Take a look at the example below to see how can we read individual tokens using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer\n",
      "Science\n",
      "is\n",
      "so\n",
      "fun\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"Computer Science is so fun!\"\n",
    "\n",
    "doc = nlp(sentence_1) # we are passing text into spacy pipeline\n",
    "\n",
    "for token in doc: # iterate through every token in doc\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger\n",
    "\n",
    "Tagger object is responsible for attaching to each token an attribute about its **part of speech (POS)** tag.\n",
    "We can look at the same example as before, but this time we'll print the tag attribute of each token as well. \n",
    "We'll use f-string to make printing prettier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token     Token tag\n",
      "Computer  15308085513773655218\n",
      "Science   15794550382381185553\n",
      "is        13927759927860985106\n",
      "so        164681854541413346\n",
      "fun       10554686591937588953\n",
      "!         12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':<10}{'Token tag'}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10}{token.tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong? Why token tag displays a long number instead a description of a part of speech?\n",
    "Spacy uses token.tag to show the unique identifier for a particular part of speech.\n",
    "But we can use token.tag_ to view something easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token     Token tag\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'explain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken tag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;241m.\u001b[39mtag_\u001b[38;5;241m.\u001b[39mexplain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'explain'"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':<10}{'Token tag'}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10}{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "Dependency Parser is used to retrieve relations between individual tokens, like subject or object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show displacy dep parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "\n",
    "Lemmatization assigns each token that exists in model vocabulary its base form.\n",
    "Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"been\")[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'present'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"presents\")[0].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognizer (NER)\n",
    "\n",
    "Recognizing named entities is one of the most powerfool functionalities of spacy library. Spacy models have been trained to classify some words as entities like persons, companies or locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates          PERSON              People, including fictional\n",
      "Microsoft           ORG                 Companies, agencies, institutions, etc.\n",
      "2021                DATE                Absolute or relative dates or periods\n",
      "2$                  MONEY               Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bill Gates has founded Microsoft in 2021. It cost him 2$\")\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text:<20}{entity.label_:<20}{str(spacy.explain(entity.label_)):<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Named Entity Recognizer successfully recognized a span of two tokens \"Bill Gates\" as a PERSON, \"Microsoft\" as an ORG for organization, 2021 as a DATE and 2$ as MONEY.\n",
    "\n",
    "For full list, visit: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Interesting sentence \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    last year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"Interesting sentence last year\")\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features under the hood\n",
    "\n",
    "Apart from the described tools, there are a lot of other things happening under the hood. \n",
    "Spacy recognises (with no 100% accuracy) the boundaries of sentences.\n",
    "After applying spacy pipeline to text, each word is given a math representation as a vector which we will explore in the next chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jupyterquiz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyterquiz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_quiz\n\u001b[0;32m      2\u001b[0m display_quiz(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresources/quizzes/questions2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jupyterquiz'"
     ]
    }
   ],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"resources/quizzes/questions2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click [here](3.%20Let's%20Learn%20NLP%20-%20Understanding%20Word%20Vectors.ipynb) to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
