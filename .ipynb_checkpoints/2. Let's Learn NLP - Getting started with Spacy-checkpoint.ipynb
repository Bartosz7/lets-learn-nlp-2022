{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./resources/images/header_logo.png)\n",
    "\n",
    "# Chapter 2 Getting started with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to clear the cell outputs before starting this tutorial (Cell -> All Output -> Clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will explore **Spacy**, one of the most popular open-source NLP libraries. \n",
    "It is designed to hide away all of the complex algorithms and just give the user ready functions for text processing and analysis. It is also designed to work efficiently with other data science related libraries like *numpy* or *pandas*.\n",
    "\n",
    "\n",
    "This section will focus on key text processing methods, namely:\n",
    "* Tokenization\n",
    "* Part Of Speech (POS) Tagging\n",
    "* Dependency Parsing\n",
    "* Lemmatization\n",
    "* Named Entity Recognition (NER)\n",
    "\n",
    "Apart from that we will see how to iterate through sentences and use different spacy objects attributes.\n",
    "\n",
    "If you have any problems or just want to dive deeper and experiment with spacy, [**spacy documentation**](https://spacy.io/) is a great place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spacy language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start using **spacy** library, we also need to download **the language model** we will use. These models are just files that were created by automatic training using Machine Learning methods on thousands of documents.\n",
    "\n",
    "**Models** include a lot of information, for example: the vocabulary used, stop_words, vector representations of the words (more in Chapter 3) and many many more. Let's start by importing spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to load an appropriate language model. For this course, we will be using a medium-size English language model called `en_core_web_md`, but it's important to know that there are many models available. Some of them can be of general-purpose like the one we use, other may be focused on specific uses like models to analyse health-related research papers. Let's load a model. **Note:** Loading a model may take a while depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `nlp` is what we call a pipeline which loads the elements of the language model. But, what is a pipeline?\n",
    "A pipeline in software engineering is a chain of processes or functions used in a specific order. The output of each pipeline element is the input of the next one. You can read more about it [here](https://en.wikipedia.org/wiki/Pipeline_(software))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 2 Spacy Pipeline\n",
    "\n",
    "Two of the most popular lines of code using spacy are:\n",
    "```\n",
    "nlp = spacy.load(model_string_code)  # loading a model\n",
    "doc = nlp(text)                      # using the pipeline on some plain text\n",
    "```\n",
    "`nlp` is what we call a pipeline. To understand what is happening when these two lines are used, watch the video below.\n",
    "\n",
    "Click [here](https://youtu.be/n4eYyzP8cbc) to watch on YT or run the cell below for in-app view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO VIEW THE VIDEO IN JUPYTER NOTEBOOK\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(\"n4eYyzP8cbc\", width=\"100%\", height=\"500px\")\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of spacy pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the default spacy pipeline generally looks.\n",
    "\n",
    "![](./resources/images/spacy_pipeline.svg)\n",
    "\n",
    "Now that we have loaded the model and created a standard pipeline. Let's take a look what is inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of different tools included in the pipeline. We will explore some of them in more depth now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Firstly, the plain text is broken into smallest units like individual words or terms (**tokenization**). We call these smallest chunks of text **tokens**. Tokens can take form of phrases, individual words, numerals, punctuation marks, whitespaces and more. Take a look at the example below to see how can we read individual tokens using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"Computer Science is so fun!\"\n",
    "doc = nlp(sentence_1) # we are passing text into spacy pipeline\n",
    "\n",
    "for token in doc: # iterate over every token in doc\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask: Why do we need tokenization if we can just split the sentence into words? Answer: It's not the same.\n",
    "\n",
    "Remember: **tokens are not necessarily words**. They are usually the smallest units of information for text.\n",
    "Look at the example sentence below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_2 = \"They're driving a car worth $100000 for 10km.\"\n",
    "doc = nlp(sentence_2)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see although `They're` is a continuous text string, however it is tokenised into two tokens `They` and `'re`. Spacy is also intelligent enough to see that `100000` is a number whereas `$` means a currency and to see that `10` is a number and `km` means a unit of measurement.\n",
    "\n",
    "### Using the doc object\n",
    "The doc object is the output of the pipeline used on plain text. It can be used like a list of tokens, which you can access through standard Python indexing (starts at 0), see examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0])         # gets the token at 1st position (Python indexing starts at 0)\n",
    "print(type(doc[0]))   # the type of doc[0] is a single token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of tokens is called a Span. It is yet another object used by spacy and it has its own attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[2:5])       # gets the tokens starting from 3rd ending with 5th token \n",
    "print(type(doc[2:5])) # the type of doc[2:5] is a span, a span is a collection of multiple tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline recognises (with various accuracy) the **boundaries of sentence**. That is why apart from iterating over tokens, we can iterate through whole sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is very interesting. I want to become a Computer Scientist. I wonder how hard it is to learn programming.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger\n",
    "\n",
    "Tagger object is responsible for attaching to each token an attribute about its **part of speech (POS)** tag.\n",
    "We can look at the same example as before, but this time we'll print the tag attribute of each token as well. \n",
    "We'll use f-string to make printing prettier. If you don't know f-strings, take a look at this short tutorial at Freecodecamp: https://www.freecodecamp.org/news/python-f-strings-tutorial-how-to-use-f-strings-for-string-formatting/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong? Why token tag displays a long number instead a description of a part of speech?\n",
    "Spacy uses `token.tag` to show the unique identifier for a particular part of speech (this design feature is used in many spacy attributes).\n",
    "But we can use `token.tag_` to view its string id which is something easier to interpret. Let's revise our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better, but unless you are a linguist you may not really now what these string codes mean. Fortunately, spacy also has a special method `explain` that is used to further explain functions and string codes like this. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text} - {spacy.explain(token.tag_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can clearly see what Part Of Speech (POS) each token represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "Dependency Parser is used to retrieve **relations (dependencies)** between tokens, like subject or object. We won't use it much during this workshop, but you can play around with it by changing the sentence and see how many dependencies can spacy recognise. But be careful, dependency graphs of longer sentences may be very wide.\n",
    "\n",
    "**Note:** We are using `displacy` library to visualise the output. We will use it again soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sentence = \"This is an example sentence.\"\n",
    "doc = nlp(sentence)\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "\n",
    "Lemmatization assigns each token that exists in model vocabulary its lemma, in other words its base form. You can say that a lemma is just a basic form of a word under which it appears in a dictionary. Note that we'll again use `lemma_` attribute to get string output rather than `lemma` which gives an integer id.\n",
    "\n",
    "Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"My turtles are faster than your rabbits.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognizer (NER)\n",
    "\n",
    "Recognizing **named entities** is one of the most satisfying functionalities of spacy library. Spacy models have been trained to classify some tokens and phrases as named entities such as people, companies or locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Bill Gates founded Microsoft in 1975 in Albuquerque. Now, its revenue is over 168 billion USD \")\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text:<25}{entity.label_:<20}{str(spacy.explain(entity.label_)):<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognizer successfully recognized a span of two tokens `Bill Gates` as a `PERSON`, `Microsoft` as an `ORG` for organization, `1975` as a `DATE`, `ALbuquerque` as a `GPE` for state and `over 168 billion USD` as `MONEY`.\n",
    "\n",
    "Let's now use `displacy` to better visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful Features\n",
    "\n",
    "Apart from the described tools, there are a lot of other useful features we can use.\n",
    "\n",
    "#### Stopwords and accessing words from vocabulary\n",
    "One of them is identification of **stopwords**. Stopwords are commonly used words in any language., for example: \"the\", \"is\", \"and\". They often do not provide any additional information, thus we want to be able to filter them out before performing NLP analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, spacy has already an inbuilt collection of over 300 stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether a particular word is a stopword or not. We have to retrieve the word from spacy vocabulary and check its `is_stop` attribute, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"hello\"].is_stop # the word hello is not a stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"but\"].is_stop # the word but is a stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add your own stopwords or change a non-stopword into a stopword if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hello into a stopword\n",
    "nlp.vocab[\"hello\"].is_stop = True # now, hello is considered a stop_word\n",
    "\n",
    "# Change 'and' into a non-stopword\n",
    "nlp.vocab[\"and\"].is_stop = False\n",
    "\n",
    "# Add a new stopword to spacy defaults\n",
    "nlp.Defaults.stop_words.add(\"FYI\")\n",
    "\n",
    "# Remove a stopword from spacy defaults\n",
    "nlp.Defaults.stop_words.remove(\"too\")\n",
    "# Be careful, if you run this cell twice KeyError will be displayed\n",
    "# as you're trying to remove a nonexisting word\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"resources/quizzes/questions2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Check the list of token attributes in spacy documentation [here](https://spacy.io/api/token). \n",
    "\n",
    "A. Write a function `count_stopwords` that will iterate through each sentence in a `doc` and print the number of stopwords that each sentence includes. Check your solution using the given doc.\n",
    "\n",
    "*Hint: you should iterate through each token in a sentence*\n",
    "\n",
    "B. After that, try to write a similar function `count_lowercase` that will count lowercase tokens in each sentence. Check your solution using the given doc.\n",
    "\n",
    "*Hint: you need other token attribute, check the documentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is the hardest exercise in my life. There is no way I can do it. Oh wait... I think I did it!\")\n",
    "\n",
    "def count_stopwords(doc):\n",
    "    pass # remove 'pass' and type your code here\n",
    "\n",
    "def count_lowercase(doc):\n",
    "    pass # remove 'pass' and type your code here\n",
    "\n",
    "# count_stopwords(doc)\n",
    "count_lowercase(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your answer by watching [this video](https://youtu.be/o4jpC5RGT0g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Continue to part 3](3.%20Let's%20Learn%20NLP%20-%20Understanding%20Word%20Vectors.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
